{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import pdist\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import seaborn as sns\n",
    "from classes.behavioural_experiment import Behavioural_experiment\n",
    "from classes.participant import Participant\n",
    "from classes.trial import Trial\n",
    "from classes.models import presets\n",
    "from methods.extraction import vecToDict, dictToVec\n",
    "from statannotations.Annotator import Annotator\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_study = open('./data/database.obj','rb')\n",
    "study = pickle.load(file_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors_2 = study.priors[study.priors.experiment == 'experiment_2']\n",
    "priors_2_implausible = priors_2[priors_2.trial_type == 'implausible']\n",
    "p2_i = priors_2_implausible[priors_2_implausible.columns[3:]].to_numpy()\n",
    "\n",
    "graph_1 = np.nan_to_num(p2_i[:, :6])\n",
    "graph_2 = np.nan_to_num(p2_i[:, 6:12])\n",
    "graph_3 = np.nan_to_num(p2_i[:, 12:])\n",
    "\n",
    "prior_graphs = graph_1 + graph_2 + graph_3\n",
    "\n",
    "np.sum(prior_graphs != 0, axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posteriors_2 = study.posteriors[study.posteriors.experiment == 'experiment_2']\n",
    "posteriors_2_implausible = posteriors_2[posteriors_2.trial_type == 'implausible']\n",
    "\n",
    "p2_i = posteriors_2_implausible[posteriors_2_implausible.columns[3:]].to_numpy()\n",
    "\n",
    "graph_1 = np.nan_to_num(p2_i[:, :6])\n",
    "graph_2 = np.nan_to_num(p2_i[:, 6:12])\n",
    "graph_3 = np.nan_to_num(p2_i[:, 12:])\n",
    "\n",
    "posterior_graphs = graph_1 + graph_2 + graph_3\n",
    "\n",
    "np.sum(posterior_graphs != 0, axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('./data/df_participants.csv')\n",
    "sample_size = len(df)\n",
    "print('Sample size:', sample_size)\n",
    "experiments = ['experiment_1', 'experiment_2', 'experiment_3', 'experiment_4']\n",
    "sample_size_exps = [len(df[df.experiment == exp]) for exp in experiments]\n",
    "print(sample_size_exps)\n",
    "\n",
    "df_trials = pd.read_csv('./data/df_trials_wprior.csv')\n",
    "\n",
    "interventions = pd.read_csv('./data/df_interventions.csv')\n",
    "\n",
    "## Demographics\n",
    "demos = pd.read_csv('./data/df_demographics.csv')\n",
    "\n",
    "experiments = df.experiment.unique().tolist()\n",
    "print(experiments)\n",
    "\n",
    "df_long_2 = pd.read_csv('./data/accuracy_lf_exp2.csv')\n",
    "df_long_2_ham = pd.read_csv('./data/editdist_lf_exp2.csv')\n",
    "\n",
    "df_long_3 = pd.read_csv('./data/accuracy_lf_exp3_wprior.csv')\n",
    "df_long_3_ham = pd.read_csv('./data/editdist_lf_exp3_wprior.csv')\n",
    "df_long_3_ham['difficulty'] = df_long_3_ham['difficulty'].replace({'congruent':1.0, 'incongruent':2.0})\n",
    "#df_long_3['hamming'] = df_long_3_ham.accuracy.to_list()\n",
    "\n",
    "pids = df_long_3.participant.unique()\n",
    "diff = ['congruent', 'incongruent']\n",
    "\n",
    "df_long_3['utid'] = df_long_3.participant + '_' + df_long_3.difficulty.astype(str)\n",
    "df_long_3_ham['utid'] = df_long_3_ham.participant + '_' + df_long_3_ham.difficulty.astype(str)\n",
    "\n",
    "df_long_3['hamming'] = np.nan\n",
    "df_long_3.loc[df_long_3.utid.isin(df_long_3_ham.utid), 'hamming'] = df_long_3_ham.loc[df_long_3_ham.utid.isin(df_long_3.utid), 'accuracy'].to_list()\n",
    "df_long_3 = df_long_3.drop(['utid'], axis=1)\n",
    "\n",
    "\n",
    "df_long_4 = pd.read_csv('./data/accuracy_lf_exp4_wprior.csv')\n",
    "df_long_4_ham = pd.read_csv('./data/editdist_lf_exp4_wprior.csv')\n",
    "df_long_4_ham['difficulty'] = df_long_4_ham['difficulty'].replace({'congruent':1.0, 'incongruent':2.0})\n",
    "\n",
    "df_long_4['utid'] = df_long_4.participant + '_' + df_long_4.difficulty.astype(str)\n",
    "df_long_4_ham['utid'] = df_long_4_ham.participant + '_' + df_long_4_ham.difficulty.astype(str)\n",
    "\n",
    "df_long_4['hamming'] = np.nan\n",
    "df_long_4.loc[df_long_4.utid.isin(df_long_4_ham.utid), 'hamming'] = df_long_4_ham.loc[df_long_4_ham.utid.isin(df_long_4.utid), 'accuracy'].to_list()\n",
    "df_long_4 = df_long_4.drop(['utid'], axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selections = [\n",
    "    ['experiment_1', 'experiment_2', 'experiment_3', 'experiment_4'],\n",
    "    ['experiment_1'],\n",
    "    ['experiment_2'],\n",
    "    ['experiment_3'],\n",
    "    ['experiment_4'],\n",
    "]\n",
    "\n",
    "for experiment in selections:\n",
    "    demos_local = demos[demos.experiment.isin(experiment)]\n",
    "    print(f'Selection: {experiment}')\n",
    "    print(f'Gender: {demos_local[demos_local.gender == 1].shape[0]} males (N = {demos_local.shape[0]})')\n",
    "    print(f'Age: {demos_local.age.mean()}, sd={demos_local.age.std()}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive and general link recovery statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\n",
    "    'experiment_1', \n",
    "    'experiment_2', \n",
    "    'experiment_3', \n",
    "    'experiment_4'\n",
    "]\n",
    "\n",
    "for experiment in experiments:\n",
    "    df_exp = df[df.experiment == experiment]\n",
    "\n",
    "    print(f'Data for {experiment}')\n",
    "    # Links recovered\n",
    "    t, p = stats.ttest_1samp(df_exp.genCorr, 1/5)\n",
    "    degf = df_exp.genCorr.size - 1\n",
    "    print(f'Links recovered: {np.round(df_exp.genCorr.mean(), 6)}, t({degf})={np.round(t, 6)}, p={np.round(p, 6)}, chance level={1/5}')\n",
    "\n",
    "    # positive links recovered\n",
    "    t, p = stats.ttest_1samp(df_exp.posCorr, 2/6)\n",
    "    degf = df_exp.posCorr.size - 1\n",
    "    print(f'Positive links recovered: {np.round(df_exp.posCorr.mean(), 6)}, t({degf})={np.round(t, 6)}, p={np.round(p, 6)}, chance level={np.round(2/6, 2)}')\n",
    "\n",
    "    # negative links recovered\n",
    "    t, p = stats.ttest_1samp(df_exp.negCorr, 2/6)\n",
    "    degf = df_exp.negCorr.size - 1\n",
    "    print(f'Negative links recovered: {np.round(df_exp.negCorr.mean(), 6)}, t({degf})={np.round(t, 6)}, p={np.round(p, 6)}, chance level={np.round(2/6, 2)}')\n",
    "\n",
    "    t, p = stats.ttest_ind(df_exp.posCorr, df_exp.negCorr)\n",
    "    degf = df_exp.posCorr.size - 1 + df_exp.negCorr.size - 1\n",
    "    print(f'Difference negative positive: diff={df_exp.posCorr.mean() - df_exp.negCorr.mean()}, t({degf})={np.round(t, 6)}, p={np.round(p, 6)}')\n",
    "    \n",
    "    # weak links recovered\n",
    "    t, p = stats.ttest_1samp(df_exp.weakCorr, 2/6)\n",
    "    degf = df_exp.weakCorr.size - 1\n",
    "    print(f'Weak links recovered: {np.round(df_exp.weakCorr.mean(), 6)}, t({degf})={np.round(t, 6)}, p={np.round(p, 6)}, chance level={np.round(2/6, 2)}')\n",
    "\n",
    "    # strong links recovered\n",
    "    t, p = stats.ttest_1samp(df_exp.stgCorr, 2/6)\n",
    "    degf = df_exp.stgCorr.size - 1\n",
    "    print(f'Strong links recovered: {np.round(df_exp.stgCorr.mean(), 6)}, t({degf})={np.round(t, 6)}, p={np.round(p, 6)}, chance level={np.round(2/6, 2)}')\n",
    "\n",
    "    t, p = stats.ttest_ind(df_exp.weakCorr, df_exp.stgCorr)\n",
    "    degf = df_exp.weakCorr.size - 1 + df_exp.stgCorr.size - 1\n",
    "    print(f'Difference weak strong: diff={df_exp.weakCorr.mean() - df_exp.stgCorr.mean()}, t({degf})={np.round(t, 6)}, p={np.round(p, 6)}')\n",
    "    \n",
    "\n",
    "    print(f'Average time taken: mean={np.round(df_exp.time_taken.mean(), 6)}, std={np.round(df_exp.time_taken.std(), 6)}')\n",
    "\n",
    "\n",
    "    # Order in trial\n",
    "    if experiment[-1] in ['2', '3', '4']:\n",
    "        if experiment[-1] == '2':\n",
    "            for i, scenario in enumerate(['finance', 'estate', 'crime']):\n",
    "                group_1 = df_exp[df_exp[f'{scenario}_cond_order'] == 1][f'{scenario}_acc']\n",
    "                group_2 = df_exp[df_exp[f'{scenario}_cond_order'] == 2][f'{scenario}_acc']\n",
    "                group_3 = df_exp[df_exp[f'{scenario}_cond_order'] == 3][f'{scenario}_acc']\n",
    "            \n",
    "                f, p = stats.f_oneway(group_1, group_2, group_3)\n",
    "                print(f'Scenario {scenario} order effect: F={np.round(f, 6)}, p={np.round(p, 6)}')\n",
    "\n",
    "        if experiment[-1] in ['3', '4']:\n",
    "            for i, scenario in enumerate(['finance', 'crime']):\n",
    "                group_1 = df_exp[df_exp[f'{scenario}_cond_order'] == 1][f'{scenario}_acc']\n",
    "                group_2 = df_exp[df_exp[f'{scenario}_cond_order'] == 2][f'{scenario}_acc']\n",
    "            \n",
    "                f, p = stats.f_oneway(group_1, group_2)\n",
    "                print(f'Scenario {scenario} order effect: F={np.round(f, 6)}, p={np.round(p, 6)}')\n",
    "\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_trials[df_trials.experiment == 'experiment_2']\n",
    "df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1 percent link recovered and accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links Recovered and accuracies (mean and std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trials1 = df_trials[df_trials.experiment == 'experiment_1']\n",
    "df_trials1.groupby('trial_name')[['genCorr', 'accuracy']].mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(df_trials1[df_trials1.trial_name == 'crime'].accuracy, df_trials1[df_trials1.trial_name == 'crime_control'].accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trials1.groupby('trial_name')[['genCorr', 'accuracy']].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain graphs (means and std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trials1.groupby(['trial_name', 'trial_spec'])[['genCorr', 'accuracy']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trials1.groupby(['trial_name', 'trial_spec'])[['genCorr', 'accuracy']].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trials1.genCorr.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trials1.groupby('trial_name').accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trials1.groupby('trial_name').genCorr.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trials1.hamming.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trials.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interventions descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Across experiments')\n",
    "print('Number of interventions:', len(interventions))\n",
    "#print('Average number of interventions per participants:', len(interventions) / sample_size)\n",
    "print('Average number of interventions per trial:', len(interventions) / (sample_size * 4))\n",
    "print(f'Mean intervention length: {interventions.length_sec.mean()}')\n",
    "print(f'Std intervention length: {interventions.length_sec.std()}')\n",
    "print(f'Mean intervening time (in %): {interventions.groupby(\"utid\").relative_length.sum().mean()}')\n",
    "print(f'Std intervening time (in %): {interventions.groupby(\"utid\").relative_length.sum().std()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, experiment in enumerate(experiments):\n",
    "    interventions_exp = interventions[interventions.experiment == experiment]\n",
    "    print(f'\\n {experiment}')\n",
    "    #print('Number of interventions:', len(interventions_exp))\n",
    "    #print('Average number of interventions per participants:', len(interventions_exp) / sample_size_exps[i])\n",
    "    print('Average number of interventions per trial:', len(interventions_exp) / (sample_size_exps[i] * 4))\n",
    "    print(f'Mean intervention length: {interventions_exp.length_sec.mean()}')\n",
    "    print(f'Std intervention length: {interventions_exp.length_sec.std()}')\n",
    "\n",
    "    print(f'Mean intervening time (in %): {interventions_exp.groupby(\"utid\").relative_length.sum().mean()}')\n",
    "    print(f'Std intervening time (in %): {interventions_exp.groupby(\"utid\").relative_length.sum().std()}')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Distribution per experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for i in range(len(experiments)):\n",
    "    plt.subplot(1, len(experiments), i+1)\n",
    "    mean_acc = df[df.experiment == experiments[i]].mean_acc\n",
    "    sns.histplot(mean_acc, kde=True, stat='density')\n",
    "    plt.title(f'Mean acc. {experiments[i]}, N = {len(mean_acc)}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = {\n",
    "    'crime': 'cornflowerblue',\n",
    "    'finance': 'darkorange',\n",
    "    'estate': 'seagreen'\n",
    "}\n",
    "\n",
    "def gen_colors(labels, color_dict):\n",
    "    return [color_dict[label] for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_2.difficulty = df_long_2.difficulty.replace([1, 2, 3], ['Congruent', 'Incongruent', 'Implausible'])\n",
    "x = 'difficulty'\n",
    "y = 'accuracy'\n",
    "hue = 'scenario'\n",
    "\n",
    "order=['Congruent', 'Incongruent', 'Implausible']\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8), sharey=True)\n",
    "\n",
    "colors = [color_dict['crime'], color_dict['estate'], color_dict['finance']]\n",
    "sns.set_palette(colors)\n",
    "sns.swarmplot(x=x, y=y, data=df_long_2, hue=hue, alpha=.4, ax=ax, order=order)\n",
    "sns.pointplot(x=x, y=y, data=df_long_2, hue=hue, dodge=0.2, join=None, ci=95, ax=ax, order=order)\n",
    "\n",
    "box_pairs = [('Congruent', 'Incongruent'), ('Congruent', 'Implausible'), ('Incongruent', 'Implausible')]\n",
    "p_values = [.0208, .00009, .0009]\n",
    "add_stat_annotation(ax, data=df_long_2, x=x, y=y, box_pairs=box_pairs, perform_stat_test=False, test_short_name=\"Tukey's range test\", pvalues=p_values, verbose=2, loc='outside', fontsize=16)\n",
    "\n",
    "sns.despine()\n",
    "ax.set_ylabel('Accuracy', fontsize=17)\n",
    "ax.set_xlabel('Difficulty', fontsize=17)\n",
    "#figure.suptitle('Difference in accuracy between difficulty conditions', fontsize=16)\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend([], [],frameon=False)\n",
    "\n",
    "plt.setp(ax.get_yticklabels(), fontsize=15)\n",
    "plt.setp(ax.get_xticklabels(), fontsize=15)\n",
    "ax.set_title('Supplementary Experiment', fontsize=17, y=1.3)\n",
    "\n",
    "ax.legend(handles[0:3], labels[0:3], labelspacing=1, loc=6, bbox_to_anchor=(1, 0.5), fontsize=17)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace = 0.1)\n",
    "plt.savefig('./plots/lmer_parts_exp2.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_2.difficulty = df_long_2.difficulty.replace([1, 2, 3], ['Congruent', 'Incongruent', 'Implausible'])\n",
    "x = 'difficulty'\n",
    "y = 'accuracy'\n",
    "hue = 'scenario'\n",
    "\n",
    "order=['Congruent', 'Incongruent', 'Implausible']\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
    "\n",
    "### EXPERIMENT 2\n",
    "df_long_3.difficulty = df_long_3.difficulty.replace([1, 2], ['Congruent', 'Incongruent'])\n",
    "x = 'difficulty'\n",
    "y = 'accuracy'\n",
    "hue = 'scenario'\n",
    "\n",
    "colors = [color_dict['crime'], color_dict['finance']]\n",
    "sns.set_palette(colors)\n",
    "sns.swarmplot(x=x, y=y, data=df_long_3, hue=hue, alpha=.4, ax=axs[0], order=order[:2])\n",
    "sns.pointplot(x=x, y=y, data=df_long_3, hue=hue, join=None, dodge=0.1, ci=95, ax=axs[0], order=order[:2])\n",
    "\n",
    "box_pairs = [('Congruent', 'Incongruent')]\n",
    "p_values = [.0140]\n",
    "#add_stat_annotation(ax=axs[0], data=df_long_3, x=x, y=y, box_pairs=box_pairs, perform_stat_test=False, test_short_name=\"Tukey's range test\", pvalues=p_values, verbose=2, loc='outside', fontsize=16)\n",
    "x1, x2 = 0, 1\n",
    "y, h, col = 1.03, 0.03, 'k'\n",
    "axs[0].plot([x1, x1, x2, x2], [y, y+h, y+h, y], color=col)\n",
    "axs[0].text((x1 + x2)*0.5, y+1.2*h, '*', ha='center', va='bottom', color=col, fontsize=17)\n",
    "\n",
    "sns.despine()\n",
    "axs[0].set_ylabel('')\n",
    "axs[0].set_xlabel('Difficulty', fontsize=17)\n",
    "axs[0].legend([], [] ,frameon=False)\n",
    "#fig.suptitle('Difference in accuracy between difficulty conditions', fontsize=16)\n",
    "\n",
    "plt.setp(axs[0].get_yticklabels(), fontsize=15)\n",
    "plt.setp(axs[0].get_xticklabels(), fontsize=15)\n",
    "axs[0].set_title('Experiment 2', fontsize=17, y=1.1)\n",
    "\n",
    "### EXPERIMETN 3\n",
    "df_long_4.difficulty = df_long_4.difficulty.replace([1, 2], ['Congruent', 'Incongruent'])\n",
    "x = 'difficulty'\n",
    "y = 'accuracy'\n",
    "hue = 'scenario'\n",
    "\n",
    "colors = [color_dict['crime'], color_dict['finance']]\n",
    "sns.set_palette(colors)\n",
    "sns.swarmplot(x=x, y=y, data=df_long_4, hue=hue, alpha=.4, ax=axs[1], order=order[:2])\n",
    "sns.pointplot(x=x, y=y, data=df_long_4, hue=hue, join=None, dodge=0.1, ci=95, ax=axs[1], order=order[:2])\n",
    "\n",
    "box_pairs = [('Congruent', 'Incongruent')]\n",
    "p_values = [.0300]\n",
    "x1, x2 = 0, 1\n",
    "y, h, col = 1.03, 0.03, 'k'\n",
    "axs[1].plot([x1, x1, x2, x2], [y, y+h, y+h, y], color=col)\n",
    "axs[1].text((x1 + x2)*0.5, y+1.2*h, '*', ha='center', va='bottom', color=col, fontsize=17)\n",
    "\n",
    "sns.despine()\n",
    "axs[1].set_ylabel('')\n",
    "axs[1].set_xlabel('Difficulty', fontsize=17)\n",
    "#fig.suptitle('Difference in accuracy between difficulty conditions', fontsize=16)\n",
    "\n",
    "handles, labels = axs[1].get_legend_handles_labels()\n",
    "\n",
    "plt.setp(axs[1].get_yticklabels(), fontsize=15)\n",
    "plt.setp(axs[1].get_xticklabels(), fontsize=15)\n",
    "axs[1].set_title('Experiment 3', fontsize=17, y=1.1)\n",
    "\n",
    "axs[1].legend(handles[0:2], labels[0:2], labelspacing=1, loc=6, bbox_to_anchor=(1, 0.5), fontsize=17)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace = 0.1)\n",
    "plt.savefig('./plots/lmer_parts.pdf')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hamming distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ee_2 = df_long_2_ham\n",
    "df_ee_2.difficulty = df_ee_2.difficulty.replace([1, 2, 3], ['Congruent', 'Incongruent', 'Implausible'])\n",
    "x = 'difficulty'\n",
    "y = 'accuracy'\n",
    "hue = 'scenario'\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(17, 8), sharey=True)\n",
    "\n",
    "colors = [color_dict['crime'], color_dict['estate'], color_dict['finance']]\n",
    "sns.set_palette(colors)\n",
    "sns.barplot(x=x, y=y, data=df_ee_2, hue=hue, ax=axs[0])\n",
    "\n",
    "box_pairs = [('Congruent', 'Incongruent'), ('Congruent', 'Implausible'), ('Incongruent', 'Implausible')]\n",
    "p_values = [.0021, .00009, .1110]\n",
    "add_stat_annotation(axs[0], data=df_ee_2, x=x, y=y, box_pairs=box_pairs, perform_stat_test=False, test_short_name=\"Tukey's range test\", pvalues=p_values, verbose=2, loc='outside')\n",
    "\n",
    "sns.despine()\n",
    "axs[0].set_ylabel('Accuracy', fontsize=17)\n",
    "axs[0].set_xlabel('Difficulty', fontsize=17)\n",
    "#figure.suptitle('Difference in accuracy between difficulty conditions', fontsize=16)\n",
    "\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "axs[0].legend([], [],frameon=False)\n",
    "axs[0].set_title('A. Experiment 2', fontsize=17, y=1.3)\n",
    "\n",
    "plt.setp(axs[0].get_yticklabels(), fontsize=15)\n",
    "plt.setp(axs[0].get_xticklabels(), fontsize=15)\n",
    "\n",
    "\n",
    "### EXPERIMETN 3\n",
    "df_ee_3 = df_long_3_ham\n",
    "df_ee_3.difficulty = df_ee_3.difficulty.replace([1, 2], ['Congruent', 'Incongruent'])\n",
    "x = 'difficulty'\n",
    "y = 'accuracy'\n",
    "hue = 'scenario'\n",
    "\n",
    "colors = [color_dict['crime'], color_dict['finance']]\n",
    "sns.set_palette(colors)\n",
    "sns.barplot(x=x, y=y, data=df_ee_3, hue=hue, ax=axs[1], order=['Congruent', 'Incongruent'])\n",
    "box_pairs = [('Congruent', 'Incongruent')]\n",
    "p_values = [.02]\n",
    "add_stat_annotation(ax=axs[1], data=df_ee_3, x=x, y=y, box_pairs=box_pairs, perform_stat_test=False, test_short_name=\"Tukey's range test\", pvalues=p_values, verbose=2, loc='outside')\n",
    "\n",
    "sns.despine()\n",
    "axs[1].set_ylabel('')\n",
    "axs[1].set_xlabel('Difficulty', fontsize=17)\n",
    "#fig.suptitle('Difference in accuracy between difficulty conditions', fontsize=16)\n",
    "\n",
    "#handles, labels = axs[1].get_legend_handles_labels()\n",
    "axs[1].legend([], frameon=False)\n",
    "\n",
    "plt.setp(axs[1].get_yticklabels(), fontsize=15)\n",
    "plt.setp(axs[1].get_xticklabels(), fontsize=15)\n",
    "axs[1].set_title('B. Experiment 3', fontsize=17, y=1.3)\n",
    "\n",
    "\n",
    "#Experiment 4\n",
    "df_long_4_ham.difficulty = df_long_4_ham.difficulty.replace([1, 2], ['Congruent', 'Incongruent'])\n",
    "x = 'difficulty'\n",
    "y = 'accuracy'\n",
    "hue = 'scenario'\n",
    "\n",
    "colors = [color_dict['crime'], color_dict['finance']]\n",
    "sns.set_palette(colors)\n",
    "sns.barplot(x=x, y=y, data=df_long_4_ham, hue=hue, ax=axs[2], order=['Congruent', 'Incongruent'])\n",
    "\n",
    "box_pairs = [('Congruent', 'Incongruent')]\n",
    "p_values = [.2084]\n",
    "add_stat_annotation(ax=axs[2], data=df_long_4_ham, x=x, y=y, box_pairs=box_pairs, perform_stat_test=False, test_short_name=\"Tukey's range test\", pvalues=p_values, verbose=2, loc='outside', fontsize=16)\n",
    "\n",
    "sns.despine()\n",
    "axs[2].set_ylabel('')\n",
    "axs[2].set_xlabel('Difficulty', fontsize=17)\n",
    "#fig.suptitle('Difference in accuracy between difficulty conditions', fontsize=16)\n",
    "\n",
    "#handles, labels = axs[1].get_legend_handles_labels()\n",
    "axs[2].legend(handles[0:3], labels[0:3], labelspacing=1, loc=6, bbox_to_anchor=(1, 0.5), fontsize=17)\n",
    "\n",
    "plt.setp(axs[2].get_yticklabels(), fontsize=15)\n",
    "plt.setp(axs[2].get_xticklabels(), fontsize=15)\n",
    "axs[2].set_title('C. Experiment 4', fontsize=17, y=1.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace = 0.1)\n",
    "plt.savefig('./plots/lmer_parts_hamming.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Priors "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same models with prior shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = 'difficulty'\n",
    "y = 'accuracy'\n",
    "hue = 'lc_prior_bf'\n",
    "prior_geq0 = [1, 0]\n",
    "titles_priors = ['Prior best fit', 'No prior best fit']\n",
    "prior_types = ['lc_prior_bf']\n",
    "model_names = ['Adaptive Selective LC']\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
    "letter = ['A', 'B']\n",
    "\n",
    "p_values_arr = [(.2061, .0092), (.3411, .0047)]\n",
    "experiments = ['experiment_3', 'experiment_4']\n",
    "for j, exp in enumerate(experiments):\n",
    "    if j == 0:\n",
    "        df_plot = df_long_3\n",
    "    else:\n",
    "        df_plot = df_long_4\n",
    "\n",
    "    df_plot.lc_prior_bf.replace({0:'Flat', 1:'Non-flat'}, inplace=True)\n",
    "    df_plot.difficulty.replace({1:'Congruent', 2:'Incongruent'}, inplace=True)\n",
    "    order = ['Congruent', 'Incongruent']\n",
    "    hue_order = ['Flat', 'Non-flat']\n",
    "    sns.stripplot(x=x, y=y, data=df_plot, hue=hue, alpha=.4, ax=axs[j], dodge=True, order=order, hue_order=hue_order)\n",
    "    sns.pointplot(x=x, y=y, data=df_plot, hue=hue, dodge=0.38, ci=95, ax=axs[j], order=order, hue_order=hue_order)\n",
    "\n",
    "    box_pairs = [\n",
    "        (('Congruent', 'Flat'), ('Incongruent', 'Flat')),\n",
    "        (('Congruent', 'Non-flat'), ('Incongruent', 'Non-flat'))\n",
    "    ]\n",
    "\n",
    "    annotator = Annotator(axs[j], box_pairs, data=df_plot, x=x, y=y, hue=hue, order=order, hue_order=hue_order, verbose=False)\n",
    "    annotator.configure(test_short_name=\"Tukey's range test\", loc='outside', show_test_name=False, text_format='star', fontsize=16)\n",
    "    annotator.set_pvalues(p_values_arr[j])\n",
    "    annotator.annotate()\n",
    "        \n",
    "    #add_stat_annotation(axs[i, j], data=df_plot, x=x, y=y, box_pairs=box_pairs, perform_stat_test=False, test_short_name=\"Tukey's range test\", pvalues=p_values, verbose=2, loc='outside')\n",
    "\n",
    "    sns.despine()\n",
    "    if j == 0:\n",
    "        axs[j].set_ylabel('Accuracy', fontsize=17)\n",
    "        axs[j].legend([], [], frameon=False)\n",
    "    else:\n",
    "        axs[j].set_ylabel('')\n",
    "        axs[j].legend(handles[0:2], labels[0:2], labelspacing=1, loc=6, bbox_to_anchor=(1, 0.5), fontsize=17, title='Prior Shape', title_fontsize=17)\n",
    "        \n",
    "    if i == len(prior_types) - 1:\n",
    "        axs[j].set_xlabel('Difficulty', fontsize=17)\n",
    "    else:\n",
    "        axs[j].set_xlabel('')\n",
    "\n",
    "    plt.setp(axs[j].get_yticklabels(), fontsize=15)\n",
    "    plt.setp(axs[j].get_xticklabels(), fontsize=15)\n",
    "\n",
    "    handles, labels = axs[j].get_legend_handles_labels()\n",
    "    #axs[j].legend(handles[0:2], labels[0:2])\n",
    "    axs[j].set_title(f'{letter[j]}. Experiment {j+2}',  fontsize=17, y=1.25)\n",
    "\n",
    "        \n",
    "\n",
    "#fig.suptitle('Effect of difficulty for participants best fit with and without prior parameter', fontsize=18, y=1.01)\n",
    "\n",
    "\n",
    "#fig.suptitle('Interaction of difficulty whether participants were best fit \\nwith a flat or non-flat prior in predicting accuracy', fontsize=17)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./plots/manip_effect_prior_bf_exp34.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_3.lc_prior_bf.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_4.lc_prior_bf.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_3.groupby(['lc_prior_bf', 'difficulty']).scenario.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_3.groupby(['lc_prior_bf', 'difficulty']).scenario.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_3.groupby(['lc_prior_bf', 'difficulty', 'scenario']).accuracy.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Hamming distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = 'difficulty'\n",
    "y = 'hamming'\n",
    "hue = 'lc_prior_bf'\n",
    "prior_geq0 = [1, 0]\n",
    "titles_priors = ['Prior best fit', 'No prior best fit']\n",
    "prior_types = ['lc_prior_bf']\n",
    "model_names = ['Adaptive Selective LC']\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 8), sharey=True)\n",
    "letter = ['A', 'B']\n",
    "\n",
    "p_values_arr = [(.1462, .0113), (.8436, .0012)]\n",
    "experiments = ['experiment_3', 'experiment_4']\n",
    "for j, exp in enumerate(experiments):\n",
    "    if j == 0:\n",
    "        df_plot = df_long_3\n",
    "    else:\n",
    "        df_plot = df_long_4\n",
    "\n",
    "    df_plot.lc_prior_bf.replace({0:'Flat', 1:'Non-flat'}, inplace=True)\n",
    "    df_plot.difficulty.replace({1:'Congruent', 2:'Incongruent'}, inplace=True)\n",
    "    order = ['Congruent', 'Incongruent']\n",
    "    hue_order = ['Flat', 'Non-flat']\n",
    "    sns.stripplot(x=x, y=y, data=df_plot, hue=hue, alpha=.4, ax=axs[j], dodge=True, order=order, hue_order=hue_order)\n",
    "    sns.pointplot(x=x, y=y, data=df_plot, hue=hue, dodge=0.38, ci=95, ax=axs[j], order=order, hue_order=hue_order)\n",
    "\n",
    "    box_pairs = [\n",
    "        (('Congruent', 'Flat'), ('Incongruent', 'Flat')),\n",
    "        (('Congruent', 'Non-flat'), ('Incongruent', 'Non-flat'))\n",
    "    ]\n",
    "\n",
    "    annotator = Annotator(axs[j], box_pairs, data=df_plot, x=x, y=y, hue=hue, order=order, hue_order=hue_order, verbose=False)\n",
    "    annotator.configure(test_short_name=\"Tukey's range test\", loc='outside', show_test_name=False, text_format='star', fontsize=16)\n",
    "    annotator.set_pvalues(p_values_arr[j])\n",
    "    annotator.annotate()\n",
    "        \n",
    "    #add_stat_annotation(axs[i, j], data=df_plot, x=x, y=y, box_pairs=box_pairs, perform_stat_test=False, test_short_name=\"Tukey's range test\", pvalues=p_values, verbose=2, loc='outside')\n",
    "\n",
    "    sns.despine()\n",
    "    if j == 0:\n",
    "        axs[j].set_ylabel('Accuracy', fontsize=17)\n",
    "        axs[j].legend([], [], frameon=False)\n",
    "    else:\n",
    "        axs[j].set_ylabel('')\n",
    "        axs[j].legend(handles[0:2], labels[0:2], labelspacing=1, loc=6, bbox_to_anchor=(1, 0.5), fontsize=17, title='Prior Shape', title_fontsize=17)\n",
    "        \n",
    "    if i == len(prior_types) - 1:\n",
    "        axs[j].set_xlabel('Difficulty', fontsize=17)\n",
    "    else:\n",
    "        axs[j].set_xlabel('')\n",
    "\n",
    "    plt.setp(axs[j].get_yticklabels(), fontsize=15)\n",
    "    plt.setp(axs[j].get_xticklabels(), fontsize=15)\n",
    "\n",
    "    handles, labels = axs[j].get_legend_handles_labels()\n",
    "    #axs[j].legend(handles[0:2], labels[0:2])\n",
    "    axs[j].set_title(f'{letter[j]}. Experiment {j+2}',  fontsize=17, y=1.20)\n",
    "\n",
    "        \n",
    "\n",
    "#fig.suptitle('Effect of difficulty for participants best fit with and without prior parameter', fontsize=18, y=1.01)\n",
    "\n",
    "\n",
    "#fig.suptitle('Interaction of difficulty whether participants were best fit \\nwith a flat or non-flat prior in predicting accuracy', fontsize=17)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./plots/manip_effect_prior_bf_exp34_hamming.pdf')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indirect links errors in labelled trials for experiment 3 and 4\n",
    "\n",
    "Mixed effect model can be found in mixed_effect_models_euclidean_norm.R, last analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_labelled = df_trials[df_trials.trial_name.isin(['crime', 'finance'])]\n",
    "\n",
    "order = ['Congruent', 'Incongruent']\n",
    "hue_order = ['Flat', 'Non-flat']\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
    "\n",
    "trials_3_prior = trials_labelled[trials_labelled.experiment == 'experiment_3']\n",
    "trials_3_prior.lc_prior_bf.replace({0:'Flat', 1:'Non-flat'}, inplace=True)\n",
    "trials_3_prior.trial_type.replace({'congruent':'Congruent', 'incongruent':'Incongruent'}, inplace=True)\n",
    "\n",
    "sns.stripplot(hue='lc_prior_bf', y='num_indirect_errors', x='trial_type', data=trials_3_prior, ax=axs[0], alpha=0.2, dodge=0.2, hue_order=hue_order, order=order)\n",
    "sns.pointplot(hue='lc_prior_bf', y='num_indirect_errors', x='trial_type', data=trials_3_prior, ax=axs[0], dodge=0.38, ci=95, hue_order=hue_order, order=order)\n",
    "\n",
    "box_pairs = [\n",
    "    (('Flat', 'Congruent'), ('Non-flat', 'Congruent')),\n",
    "    (('Flat', 'Incongruent'), ('Non-flat', 'Incongruent')),\n",
    "]\n",
    "box_pairs = [\n",
    "    (('Congruent', 'Flat'), ('Incongruent', 'Flat')),\n",
    "    (('Congruent', 'Non-flat'), ('Incongruent', 'Non-flat')),\n",
    "]\n",
    "p_values = [0.1143, 0.0080]\n",
    "\n",
    "#annotator = Annotator(axs[0], box_pairs, data=trials_3_prior, hue='lc_prior_bf', y='num_indirect_errors', x='trial_type', order=order, hue_order=hue_order, verbose=False)\n",
    "#annotator.configure(test_short_name=\"Tukey's range test\", loc='outside', show_test_name=False, text_format='star', fontsize=16)\n",
    "#annotator.set_pvalues(p_values)\n",
    "#annotator.annotate()\n",
    "\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "axs[0].legend([], [], frameon=False, fontsize=17)\n",
    "axs[0].set_title('A. Experiment 2', fontsize=17, y=1.25)\n",
    "axs[0].set_ylabel('Number of errors on indirect links',  fontsize=17)\n",
    "axs[0].set_xlabel('',  fontsize=17)\n",
    "axs[0].set_xticklabels(order,  fontsize=15)\n",
    "axs[0].set_yticklabels(axs[0].get_yticklabels(), fontsize=15)\n",
    "\n",
    "\n",
    "trials_4_prior = trials_labelled[trials_labelled.experiment == 'experiment_4']\n",
    "trials_4_prior.lc_prior_bf.replace({0:'Flat', 1:'Non-flat'}, inplace=True)\n",
    "trials_4_prior.trial_type.replace({'congruent':'Congruent', 'incongruent':'Incongruent'}, inplace=True)\n",
    "\n",
    "sns.stripplot(hue='lc_prior_bf', y='num_indirect_errors', x='trial_type', data=trials_4_prior, ax=axs[1], alpha=0.2, dodge=0.2, hue_order=hue_order, order=order)\n",
    "sns.pointplot(hue='lc_prior_bf', y='num_indirect_errors', x='trial_type', data=trials_4_prior, ax=axs[1], dodge=0.38, ci=95, hue_order=hue_order, order=order)\n",
    "\n",
    "\n",
    "p_values = [0.4514, 0.0005]\n",
    "\n",
    "annotator = Annotator(axs[1], box_pairs, data=trials_4_prior, hue='lc_prior_bf', y='num_indirect_errors', x='trial_type', order=order, hue_order=hue_order, verbose=False)\n",
    "annotator.configure(test_short_name=\"Tukey's range test\", loc='outside', show_test_name=False, text_format='star', fontsize=16)\n",
    "annotator.set_pvalues(p_values)\n",
    "annotator.annotate()\n",
    "\n",
    "handles, labels = axs[1].get_legend_handles_labels()\n",
    "axs[1].legend(handles[0:2], labels[0:2], fontsize=17, labelspacing=1, loc=6, bbox_to_anchor=(1, 0.5), title='Prior Shape', title_fontsize=17)\n",
    "axs[1].set_title('B. Experiment 3', fontsize=17, y=1.25)\n",
    "axs[1].set_ylabel('',  fontsize=17)\n",
    "axs[1].set_xlabel('',  fontsize=17)\n",
    "axs[1].set_xticklabels(order,  fontsize=15)\n",
    "#axs[1].set_yticklabels(axs[1].get_yticklabels(), fontsize=15)\n",
    "\n",
    "#fig.suptitle('Number of errors on indirect links for participants best fit with and without a non-flat prior parameter', fontsize=17)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.savefig('./plots/indirect_links_errors_34.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Root node frequencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate interventional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMport fresh datasets\n",
    "df_long_3 = pd.read_csv('./data/accuracy_lf_exp3_wprior.csv')\n",
    "df_long_4 = pd.read_csv('./data/accuracy_lf_exp4_wprior.csv')\n",
    "\n",
    "trial_types = interventions.groupby('utid').trial_type.unique().to_list()\n",
    "trial_types = np.concatenate(trial_types).flatten()\n",
    "\n",
    "utids = interventions.utid.unique()\n",
    "print(utids.size)\n",
    "df_inters = pd.DataFrame()\n",
    "new_vars_dict = {}\n",
    "new_vars = np.empty((utids.size, 50))\n",
    "new_vars[:] = np.nan\n",
    "var_order = np.empty((utids.size, 50))\n",
    "var_order[:] = np.nan\n",
    "\n",
    "var_pos = np.empty((utids.size, 50))\n",
    "var_pos[:] = np.nan\n",
    "\n",
    "var_name_pos = np.empty((utids.size, 50)).astype(str)\n",
    "var_root_pos = np.empty((utids.size, 50)).astype(str)\n",
    "var_gtroot_pos = np.empty((utids.size, 50)).astype(str)\n",
    "\n",
    "nums = []\n",
    "for i, unique_trial in enumerate(utids):\n",
    "    df_chunk = interventions[interventions.utid == unique_trial].sort_values('int_num')\n",
    "    new_var = []\n",
    "    intervened_var = []\n",
    "    var_positions = []\n",
    "    for j, inter in enumerate(df_chunk.index):\n",
    "        var_touched = df_chunk.loc[inter, 'variable']  \n",
    "        var_prior_type = df_chunk.loc[inter, 'prior_type']\n",
    "        var_gt_type = df_chunk.loc[inter, 'gt_type']\n",
    "\n",
    "        if var_touched not in intervened_var:\n",
    "            intervened_var.append(var_touched)\n",
    "            new_var.append(1)\n",
    "            new_vars[i, j] = 1\n",
    "        else:\n",
    "            new_var.append(0)\n",
    "            new_vars[i, j] = 0\n",
    "\n",
    "        \n",
    "        var_place = df_chunk.loc[inter, 'var_position']\n",
    "        var_pos[i, j] = var_place + 1\n",
    "        var_name_pos[i, j] = var_touched\n",
    "        var_root_pos[i, j] = var_prior_type\n",
    "        var_gtroot_pos[i, j] = var_gt_type\n",
    "        if not var_positions:\n",
    "            if var_place == 0:\n",
    "                var_order[i, j] = 1\n",
    "            else:\n",
    "                var_order[i, j] = 0\n",
    "            \n",
    "        elif var_place >= var_positions[-1]:\n",
    "            var_order[i, j] = 1\n",
    "        else:\n",
    "            var_order[i, j] = 0\n",
    "            \n",
    "        var_positions.append(var_place)\n",
    "    \n",
    "    nums.append(len(new_var))\n",
    "    \n",
    "    new_vars_dict[unique_trial] = new_var\n",
    "\n",
    "\n",
    "experiments = [int(utid[0]) for utid in utids]\n",
    "pids = [utid.split('-')[1] for utid in utids]\n",
    "\n",
    "prior_bf = np.zeros(len(pids))\n",
    "for i, pid in enumerate(pids):\n",
    "    if pid in df_long_3.participant.to_list():\n",
    "        prior_bf[i] = df_long_3.loc[df_long_3.participant == pid, 'lc_prior_bf'].to_list()[0]\n",
    "    elif pid in df_long_4.participant.to_list():\n",
    "        prior_bf[i] = df_long_4.loc[df_long_4.participant == pid, 'lc_prior_bf'].to_list()[0]\n",
    "    else:\n",
    "        prior_bf[i] = np.nan\n",
    "\n",
    "\n",
    "df_order = pd.DataFrame(columns=['experiment', 'utid', 'pid', 'prior_bf', 'trial_type'] + [f'inter_{i}' for i in range(35)])\n",
    "df_order['experiment'] = experiments\n",
    "df_order['utid'] = utids\n",
    "df_order['pid'] = pids\n",
    "df_order['prior_bf'] = prior_bf\n",
    "df_order['trial_type'] = trial_types\n",
    "\n",
    "df_order_name = pd.DataFrame(columns=['experiment', 'utid', 'pid', 'prior_bf', 'trial_type'] + [f'inter_{i}' for i in range(35)])\n",
    "df_order_name['experiment'] = experiments\n",
    "df_order_name['utid'] = utids\n",
    "df_order_name['pid'] = pids\n",
    "df_order_name['prior_bf'] = prior_bf\n",
    "df_order_name['trial_type'] = trial_types\n",
    "\n",
    "df_order_root = pd.DataFrame(columns=['experiment', 'utid', 'pid', 'prior_bf', 'trial_type'] + [f'inter_{i}' for i in range(35)])\n",
    "df_order_root['experiment'] = experiments\n",
    "df_order_root['utid'] = utids\n",
    "df_order_root['pid'] = pids\n",
    "df_order_root['prior_bf'] = prior_bf\n",
    "df_order_root['trial_type'] = trial_types\n",
    "\n",
    "df_order_gtroot = pd.DataFrame(columns=['experiment', 'utid', 'pid', 'prior_bf', 'trial_type'] + [f'inter_{i}' for i in range(35)])\n",
    "df_order_gtroot['experiment'] = experiments\n",
    "df_order_gtroot['utid'] = utids\n",
    "df_order_gtroot['pid'] = pids\n",
    "df_order_gtroot['prior_bf'] = prior_bf\n",
    "df_order_gtroot['trial_type'] = trial_types\n",
    "\n",
    "for i, col in enumerate([f'inter_{i}' for i in range(35)]):\n",
    "    df_order.loc[df_order.index, col ] = var_pos[:, i]\n",
    "    df_order_name.loc[df_order_name.index, col ] = var_name_pos[:, i]\n",
    "    df_order_root.loc[df_order_root.index, col] = var_root_pos[:, i]\n",
    "    df_order_gtroot.loc[df_order_gtroot.index, col] = var_gtroot_pos[:, i]\n",
    "    #print(var_root_pos[:, i])\n",
    "\n",
    "df_order_new = pd.DataFrame(columns=['experiment', 'utid', 'pid', 'prior_bf', 'trial_type'] + [f'inter_{i}' for i in range(35)])\n",
    "\n",
    "df_order_new['experiment'] = experiments\n",
    "df_order_new['utid'] = utids\n",
    "df_order_new['pid'] = pids\n",
    "df_order_new['prior_bf'] = prior_bf\n",
    "df_order_new['trial_type'] = trial_types\n",
    "for i, col in enumerate([f'inter_{i}' for i in range(35)]):\n",
    "    df_order_new.loc[df_order_new.index, col ] = new_vars[:, i]\n",
    "\n",
    "\n",
    "label_trials = ['label', 'congruent', 'incongruent', 'implausible']\n",
    "df_order.trial_type.unique()\n",
    "generic_trials = ['generic_0', 'generic_1', 'generic_2', 'generic_3']\n",
    "\n",
    "df_order_generic = df_order[df_order.trial_type.isin(generic_trials)]\n",
    "df_order_labels = df_order[df_order.trial_type.isin(label_trials)]\n",
    "\n",
    "df_order_new_generic = df_order_new[df_order_new.trial_type.isin(generic_trials)]\n",
    "df_order_new_labels = df_order_new[df_order_new.trial_type.isin(label_trials)]\n",
    "\n",
    "df_c = df_order[df_order.trial_type == label_trials[1]]\n",
    "df_congruent_3 = df_c[df_c.experiment == 3]\n",
    "df_i =  df_order[df_order.trial_type == label_trials[2]]\n",
    "df_incongruent_3 = df_i[df_i.experiment == 3]\n",
    "\n",
    "\n",
    "idx_inter = 15\n",
    "K = 3\n",
    "count_generic = np.zeros((K, idx_inter))\n",
    "count_label = np.zeros((K, idx_inter))\n",
    "\n",
    "count_variable = np.zeros((2, K, idx_inter))\n",
    "\n",
    "count_var_exp3 = np.zeros((2, K, idx_inter))\n",
    "\n",
    "\n",
    "for i, col in enumerate(df_order_generic.columns[3:3+idx_inter]):\n",
    "    for k in range(K):\n",
    "        count_generic[k, i] = df_order_generic[df_order_generic[f'inter_{i}'] == k+1].shape[0]\n",
    "        count_label[k, i] = df_order_labels[df_order_labels[f'inter_{i}'] == k+1].shape[0]\n",
    "\n",
    "        count_variable[0, k, i] = df_order_generic[df_order_generic[f'inter_{i}'] == k+1].shape[0]\n",
    "        count_variable[1, k, i] = df_order_labels[df_order_labels[f'inter_{i}'] == k+1].shape[0]\n",
    "\n",
    "        count_var_exp3[0, k, i] = df_congruent_3[df_congruent_3[f'inter_{i}'] == k+1].shape[0]\n",
    "        count_var_exp3[1, k, i] = df_incongruent_3[df_incongruent_3[f'inter_{i}'] == k+1].shape[0]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df_order_gtroot[(df_order_gtroot == 'root') | (df_order_gtroot == 'leaf')]\n",
    "new_df[['experiment', 'utid', 'pid', 'prior_bf', 'trial_type']] = df_order_root[['experiment', 'utid', 'pid', 'prior_bf', 'trial_type']]\n",
    "\n",
    "df_root_counts = new_df[new_df.experiment.isin([3, 4])].replace({'nan':np.nan})\n",
    "\n",
    "df_root_counts.prior_bf.replace({0:'flat', 1:'non flat'}, inplace=True)\n",
    "\n",
    "prior_bfs = ['non flat']\n",
    "trials = ['congruent', 'incongruent']\n",
    "experiment_idx = [3, 4]\n",
    "\n",
    "loc_idx_inter = 7\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, constrained_layout=True, figsize=(14, 7), sharex=True)\n",
    "\n",
    "hue_order = ['root', 'leaf']\n",
    "\n",
    "for i, trial in enumerate(trials):\n",
    "    df_root_counts_trial = df_root_counts[df_root_counts.trial_type == trial]\n",
    "\n",
    "    df_root_trial_prior = df_root_counts_trial[df_root_counts_trial.prior_bf == prior_bfs[0]]\n",
    "\n",
    "    for j, exp_idx in enumerate(experiment_idx):\n",
    "        #print(exp_idx)\n",
    "        df_root_trial_prior_exp = df_root_trial_prior[df_root_trial_prior.experiment == exp_idx]\n",
    "        #print(df_root_trial_prior_exp.shape)\n",
    "        \n",
    "        #print(df_root_trial_prior.shape)\n",
    "\n",
    "        mapper = {k:v for k, v in zip(df_root_counts_trial.columns[5:loc_idx_inter+5], np.arange(df_root_counts_trial.columns[5:loc_idx_inter+5].size))}\n",
    "        df_counts_plot_c = df_root_trial_prior_exp[df_root_trial_prior_exp.columns[5:loc_idx_inter + 5]].rename(mapper, axis=1)\n",
    "        df_counts_plot = df_counts_plot_c.melt()\n",
    "\n",
    "        \n",
    "        rows = ['root', 'leaf']\n",
    "        df_final = pd.DataFrame(index=rows, columns=df_counts_plot_c.columns)\n",
    "        for col in df_counts_plot_c.columns:\n",
    "            counts = df_counts_plot_c[col].value_counts()\n",
    "\n",
    "            #print(counts)\n",
    "            #print(counts.loc['root'])\n",
    "            for r in rows:\n",
    "                if r in counts.index:\n",
    "                    df_final.loc[r, col] = counts.loc[r]\n",
    "\n",
    "\n",
    "        df_final.transpose().plot(kind='bar', stacked=True, ax=axs[i, j])\n",
    "\n",
    "\n",
    "        #sns.countplot(x='variable', hue='value', data=df_counts_plot.dropna(), hue_order=hue_order, ax=axs[i, j])\n",
    "        #df_counts_plot.plot(kind='bar', stacked=True, ax=axs[i, j])\n",
    "        if i == 0:\n",
    "            axs[i, j].set_title(f'Experiment {exp_idx}, Prior shape: {prior_bfs[0]} \\n\\n Trial: {trial.capitalize()}', fontsize=19)\n",
    "        else:\n",
    "            axs[i, j].set_title(f'Trial: {trial.capitalize()}', fontsize=19)\n",
    "            axs[i, j].set_xlabel('Index of intervention', fontsize=17)\n",
    "\n",
    "        handles, labels = axs[i,j].get_legend_handles_labels()\n",
    "        if j == 0:\n",
    "            axs[i, j].set_ylabel('Intervention counts', fontsize=17)\n",
    "            axs[i, j].legend([], [],frameon=False)\n",
    "\n",
    "        if i == 0:\n",
    "            axs[i, j].legend([], [],frameon=False)\n",
    "            \n",
    "        plt.setp(axs[i, j].get_yticklabels(), fontsize=15)\n",
    "        plt.setp(axs[i, j].get_xticklabels(), fontsize=15, rotation=0)\n",
    "\n",
    "        \n",
    "\n",
    "sns.despine()\n",
    "        \n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "axs[1, 1].legend(handles[0:2], labels[0:2], labelspacing=1, loc=6, bbox_to_anchor=(1, 1.2), fontsize=17)\n",
    "plt.savefig('./plots/interventions_roots.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
